{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS ( Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors.\n",
    "# It is particularly useful for applications like nearest neighbor search in high-dimensional spaces.\n",
    "# It contains algorithm that searches in set of vectors of any size, up to billions of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97838f49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed177c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19dad6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1281, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"text.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3117e0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'text.txt'}, page_content='Readme'),\n",
       " Document(metadata={'source': 'text.txt'}, page_content=\"Sagemaker integration\\nWhen SageMaker starts a container, it will invoke the container with an argument of either\\xa0train\\xa0or\\xa0serve. We have set this container to accept the\\xa0serve\\xa0command which will, in turn, run our application.\\n* serve: The wrapper that starts the inference server. In most cases, you can use this file as-is.\\n* wsgi.py: The start up shell for the individual server workers. This only needs to be changed if you changed where predict.py is located or is named.\\n* predict.py: The algorithm-specific inference server. This is the file that you modify with your own algorithm's code.\\n* nginx.conf: Now included within the mdf-base-image. The configuration for the nginx master server that manages the multiple workers.\\nMock Interview Transcript â€“ NPTB Recommender System\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Hi Prathmesh, thanks for joining. Letâ€™s start with a quick overview of the NPTB recommender system you worked on at TCS.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Sure! NPTB, or Next Product to Buy, is a product recommendation engine built using association rule mining. Its goal is to suggest the next likely item a customer might purchase based on their current cart or historical transactions. It was developed to support the email marketing campaign use case and was deployed via Docker on AWS SageMaker.\"),\n",
       " Document(metadata={'source': 'text.txt'}, page_content=\"ğŸ§‘\\u200dğŸ’¼ Interviewer: What kind of business problem was this model solving?\\nğŸ§‘\\u200dğŸ’» Prathmesh: It was solving the cross-sell problem. The business wanted to improve customer engagement and increase the average basket size by recommending relevant products. This improves personalization and drives more sales, particularly in digital channels like emails.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: How did you handle data processing and preparation for this model?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We started with multi-year transaction data. The pipeline filtered out irrelevant SKUs like 'BATTERY TAX', replaced obsolete SKUs using a supersession table, and grouped items by transaction ID. We excluded commercial bulk purchases to remove noise. Then, we created item combinations (powersets) for baskets of up to 10 items and computed rule sets using these.\"),\n",
       " Document(metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: Can you explain association rules and how the metricsâ€”support, confidence, and liftâ€”work?\\nğŸ§‘\\u200dğŸ’» Prathmesh: Absolutely. An association rule like {A, B} â†’ C means if a customer buys A and B, they are likely to buy C.\\n* Support is how frequently the rule appears in the dataset.\\n* Confidence is the probability that C is bought when A and B are bought.\\n* Lift shows how much more likely C is to be bought with A and B compared to its overall frequency. These metrics help rank and filter the most effective rules.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: How did you deploy the NPTB model?\\nğŸ§‘\\u200dğŸ’» Prathmesh: The model was encapsulated in a Docker container and deployed on an EC2 instance managed by AWS SageMaker. We used the Model Deployment Framework (MDF) for deployment and serving, which helped with standardization and API exposure. Requests to the model were made via an API Gateway that invoked a Lambda function connected to SageMaker.'),\n",
       " Document(metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: What were some of the technical challenges in this project?\\nğŸ§‘\\u200dğŸ’» Prathmesh: One challenge was managing memory usage and load timeâ€”since the model was 23GB, it took around 160 seconds to load initially. We used warm-started containers and lazy loading strategies to minimize latency. Handling the sparsity of item combinations was another challenge, which we addressed by using product-level fallbacks for recommendations.\\n\\n\\nTo deal with sparsity in SKU-based association rules, we created a fallback mechanism. If a userâ€™s basket had no matching SKU-level rules, we mapped those SKUs to their product categories or descriptions and used a secondary set of association rules trained at that level. This allowed us to preserve recommendation quality even in edge cases.â€'),\n",
       " Document(metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: How was security and access control handled?\\nğŸ§‘\\u200dğŸ’» Prathmesh: All requests went through the API Gateway secured by TLS and authenticated via API keys. AWS IAM roles were used to restrict access between Lambda and SageMaker, and the EC2 instance was encrypted at rest. No public access was allowed to the internal Docker container.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: How did you evaluate model performance?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We used top-N hit rates as evaluation metricsâ€”specifically top-10 and top-2. These measure whether the actual next purchased item was present in the modelâ€™s top-N recommendations. With vehicle fitment filtering, the top-10 hit rate improved from ~35.7% to ~43.4%, showing strong gains.'),\n",
       " Document(metadata={'source': 'text.txt'}, page_content=\"ğŸ§‘\\u200dğŸ’¼ Interviewer: What enhancements were proposed for NPTB?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We planned to migrate from Pig scripts to PySpark for scalability, introduce OpenShift for container orchestration, and integrate higher-level product description-based rules to reduce sparsity. Also, we aimed to enable multi-channel support for website and mobile personalization.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Great! And finally, if you had to scale NPTB for a website with high traffic, what would you consider?\\nğŸ§‘\\u200dğŸ’» Prathmesh: Iâ€™d focus on horizontal scaling by autoscaling EC2 instances behind a load balancer. I'd also optimize the model size and use Redis or similar tools for real-time caching. Transitioning to a microservices architecture would help separate model serving from data processing, improving response times and maintainability.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Excellent, Prathmesh! Thank you for the detailed responses. That wraps up the technical round.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Thank you! I really enjoyed discussing the project.\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b55f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = OllamaEmbeddings(model=\"gemma:2b\")\n",
    "db = FAISS.from_documents(docs, embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a914340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f44c3015-d6f3-4c1d-a1e1-879da5c20e27', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: What were some of the technical challenges in this project?\\nğŸ§‘\\u200dğŸ’» Prathmesh: One challenge was managing memory usage and load timeâ€”since the model was 23GB, it took around 160 seconds to load initially. We used warm-started containers and lazy loading strategies to minimize latency. Handling the sparsity of item combinations was another challenge, which we addressed by using product-level fallbacks for recommendations.\\n\\n\\nTo deal with sparsity in SKU-based association rules, we created a fallback mechanism. If a userâ€™s basket had no matching SKU-level rules, we mapped those SKUs to their product categories or descriptions and used a secondary set of association rules trained at that level. This allowed us to preserve recommendation quality even in edge cases.â€'),\n",
       " Document(id='aa6f0617-b73c-4f7e-a062-0f416f4137ab', metadata={'source': 'text.txt'}, page_content=\"Sagemaker integration\\nWhen SageMaker starts a container, it will invoke the container with an argument of either\\xa0train\\xa0or\\xa0serve. We have set this container to accept the\\xa0serve\\xa0command which will, in turn, run our application.\\n* serve: The wrapper that starts the inference server. In most cases, you can use this file as-is.\\n* wsgi.py: The start up shell for the individual server workers. This only needs to be changed if you changed where predict.py is located or is named.\\n* predict.py: The algorithm-specific inference server. This is the file that you modify with your own algorithm's code.\\n* nginx.conf: Now included within the mdf-base-image. The configuration for the nginx master server that manages the multiple workers.\\nMock Interview Transcript â€“ NPTB Recommender System\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Hi Prathmesh, thanks for joining. Letâ€™s start with a quick overview of the NPTB recommender system you worked on at TCS.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Sure! NPTB, or Next Product to Buy, is a product recommendation engine built using association rule mining. Its goal is to suggest the next likely item a customer might purchase based on their current cart or historical transactions. It was developed to support the email marketing campaign use case and was deployed via Docker on AWS SageMaker.\"),\n",
       " Document(id='dbaecf8a-637a-40e3-b963-a898022a862c', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: How was security and access control handled?\\nğŸ§‘\\u200dğŸ’» Prathmesh: All requests went through the API Gateway secured by TLS and authenticated via API keys. AWS IAM roles were used to restrict access between Lambda and SageMaker, and the EC2 instance was encrypted at rest. No public access was allowed to the internal Docker container.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: How did you evaluate model performance?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We used top-N hit rates as evaluation metricsâ€”specifically top-10 and top-2. These measure whether the actual next purchased item was present in the modelâ€™s top-N recommendations. With vehicle fitment filtering, the top-10 hit rate improved from ~35.7% to ~43.4%, showing strong gains.'),\n",
       " Document(id='20d36c7c-38d9-4083-80f1-f32e362a43e7', metadata={'source': 'text.txt'}, page_content=\"ğŸ§‘\\u200dğŸ’¼ Interviewer: What enhancements were proposed for NPTB?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We planned to migrate from Pig scripts to PySpark for scalability, introduce OpenShift for container orchestration, and integrate higher-level product description-based rules to reduce sparsity. Also, we aimed to enable multi-channel support for website and mobile personalization.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Great! And finally, if you had to scale NPTB for a website with high traffic, what would you consider?\\nğŸ§‘\\u200dğŸ’» Prathmesh: Iâ€™d focus on horizontal scaling by autoscaling EC2 instances behind a load balancer. I'd also optimize the model size and use Redis or similar tools for real-time caching. Transitioning to a microservices architecture would help separate model serving from data processing, improving response times and maintainability.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Excellent, Prathmesh! Thank you for the detailed responses. That wraps up the technical round.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Thank you! I really enjoyed discussing the project.\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How was the data gathered?\"\n",
    "docs = db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ccb2ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ§‘\\u200dğŸ’¼ Interviewer: What were some of the technical challenges in this project?\\nğŸ§‘\\u200dğŸ’» Prathmesh: One challenge was managing memory usage and load timeâ€”since the model was 23GB, it took around 160 seconds to load initially. We used warm-started containers and lazy loading strategies to minimize latency. Handling the sparsity of item combinations was another challenge, which we addressed by using product-level fallbacks for recommendations.\\n\\n\\nTo deal with sparsity in SKU-based association rules, we created a fallback mechanism. If a userâ€™s basket had no matching SKU-level rules, we mapped those SKUs to their product categories or descriptions and used a secondary set of association rules trained at that level. This allowed us to preserve recommendation quality even in edge cases.â€'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04b2c20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f44c3015-d6f3-4c1d-a1e1-879da5c20e27', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: What were some of the technical challenges in this project?\\nğŸ§‘\\u200dğŸ’» Prathmesh: One challenge was managing memory usage and load timeâ€”since the model was 23GB, it took around 160 seconds to load initially. We used warm-started containers and lazy loading strategies to minimize latency. Handling the sparsity of item combinations was another challenge, which we addressed by using product-level fallbacks for recommendations.\\n\\n\\nTo deal with sparsity in SKU-based association rules, we created a fallback mechanism. If a userâ€™s basket had no matching SKU-level rules, we mapped those SKUs to their product categories or descriptions and used a secondary set of association rules trained at that level. This allowed us to preserve recommendation quality even in edge cases.â€'),\n",
       " Document(id='aa6f0617-b73c-4f7e-a062-0f416f4137ab', metadata={'source': 'text.txt'}, page_content=\"Sagemaker integration\\nWhen SageMaker starts a container, it will invoke the container with an argument of either\\xa0train\\xa0or\\xa0serve. We have set this container to accept the\\xa0serve\\xa0command which will, in turn, run our application.\\n* serve: The wrapper that starts the inference server. In most cases, you can use this file as-is.\\n* wsgi.py: The start up shell for the individual server workers. This only needs to be changed if you changed where predict.py is located or is named.\\n* predict.py: The algorithm-specific inference server. This is the file that you modify with your own algorithm's code.\\n* nginx.conf: Now included within the mdf-base-image. The configuration for the nginx master server that manages the multiple workers.\\nMock Interview Transcript â€“ NPTB Recommender System\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Hi Prathmesh, thanks for joining. Letâ€™s start with a quick overview of the NPTB recommender system you worked on at TCS.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Sure! NPTB, or Next Product to Buy, is a product recommendation engine built using association rule mining. Its goal is to suggest the next likely item a customer might purchase based on their current cart or historical transactions. It was developed to support the email marketing campaign use case and was deployed via Docker on AWS SageMaker.\"),\n",
       " Document(id='dbaecf8a-637a-40e3-b963-a898022a862c', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: How was security and access control handled?\\nğŸ§‘\\u200dğŸ’» Prathmesh: All requests went through the API Gateway secured by TLS and authenticated via API keys. AWS IAM roles were used to restrict access between Lambda and SageMaker, and the EC2 instance was encrypted at rest. No public access was allowed to the internal Docker container.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: How did you evaluate model performance?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We used top-N hit rates as evaluation metricsâ€”specifically top-10 and top-2. These measure whether the actual next purchased item was present in the modelâ€™s top-N recommendations. With vehicle fitment filtering, the top-10 hit rate improved from ~35.7% to ~43.4%, showing strong gains.'),\n",
       " Document(id='20d36c7c-38d9-4083-80f1-f32e362a43e7', metadata={'source': 'text.txt'}, page_content=\"ğŸ§‘\\u200dğŸ’¼ Interviewer: What enhancements were proposed for NPTB?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We planned to migrate from Pig scripts to PySpark for scalability, introduce OpenShift for container orchestration, and integrate higher-level product description-based rules to reduce sparsity. Also, we aimed to enable multi-channel support for website and mobile personalization.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Great! And finally, if you had to scale NPTB for a website with high traffic, what would you consider?\\nğŸ§‘\\u200dğŸ’» Prathmesh: Iâ€™d focus on horizontal scaling by autoscaling EC2 instances behind a load balancer. I'd also optimize the model size and use Redis or similar tools for real-time caching. Transitioning to a microservices architecture would help separate model serving from data processing, improving response times and maintainability.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Excellent, Prathmesh! Thank you for the detailed responses. That wraps up the technical round.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Thank you! I really enjoyed discussing the project.\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve = db.as_retriever()\n",
    "retrieve.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d0b4121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='f44c3015-d6f3-4c1d-a1e1-879da5c20e27', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: What were some of the technical challenges in this project?\\nğŸ§‘\\u200dğŸ’» Prathmesh: One challenge was managing memory usage and load timeâ€”since the model was 23GB, it took around 160 seconds to load initially. We used warm-started containers and lazy loading strategies to minimize latency. Handling the sparsity of item combinations was another challenge, which we addressed by using product-level fallbacks for recommendations.\\n\\n\\nTo deal with sparsity in SKU-based association rules, we created a fallback mechanism. If a userâ€™s basket had no matching SKU-level rules, we mapped those SKUs to their product categories or descriptions and used a secondary set of association rules trained at that level. This allowed us to preserve recommendation quality even in edge cases.â€'),\n",
       "  np.float32(2844.3572)),\n",
       " (Document(id='aa6f0617-b73c-4f7e-a062-0f416f4137ab', metadata={'source': 'text.txt'}, page_content=\"Sagemaker integration\\nWhen SageMaker starts a container, it will invoke the container with an argument of either\\xa0train\\xa0or\\xa0serve. We have set this container to accept the\\xa0serve\\xa0command which will, in turn, run our application.\\n* serve: The wrapper that starts the inference server. In most cases, you can use this file as-is.\\n* wsgi.py: The start up shell for the individual server workers. This only needs to be changed if you changed where predict.py is located or is named.\\n* predict.py: The algorithm-specific inference server. This is the file that you modify with your own algorithm's code.\\n* nginx.conf: Now included within the mdf-base-image. The configuration for the nginx master server that manages the multiple workers.\\nMock Interview Transcript â€“ NPTB Recommender System\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Hi Prathmesh, thanks for joining. Letâ€™s start with a quick overview of the NPTB recommender system you worked on at TCS.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Sure! NPTB, or Next Product to Buy, is a product recommendation engine built using association rule mining. Its goal is to suggest the next likely item a customer might purchase based on their current cart or historical transactions. It was developed to support the email marketing campaign use case and was deployed via Docker on AWS SageMaker.\"),\n",
       "  np.float32(3213.442)),\n",
       " (Document(id='dbaecf8a-637a-40e3-b963-a898022a862c', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: How was security and access control handled?\\nğŸ§‘\\u200dğŸ’» Prathmesh: All requests went through the API Gateway secured by TLS and authenticated via API keys. AWS IAM roles were used to restrict access between Lambda and SageMaker, and the EC2 instance was encrypted at rest. No public access was allowed to the internal Docker container.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: How did you evaluate model performance?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We used top-N hit rates as evaluation metricsâ€”specifically top-10 and top-2. These measure whether the actual next purchased item was present in the modelâ€™s top-N recommendations. With vehicle fitment filtering, the top-10 hit rate improved from ~35.7% to ~43.4%, showing strong gains.'),\n",
       "  np.float32(3559.1875)),\n",
       " (Document(id='20d36c7c-38d9-4083-80f1-f32e362a43e7', metadata={'source': 'text.txt'}, page_content=\"ğŸ§‘\\u200dğŸ’¼ Interviewer: What enhancements were proposed for NPTB?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We planned to migrate from Pig scripts to PySpark for scalability, introduce OpenShift for container orchestration, and integrate higher-level product description-based rules to reduce sparsity. Also, we aimed to enable multi-channel support for website and mobile personalization.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Great! And finally, if you had to scale NPTB for a website with high traffic, what would you consider?\\nğŸ§‘\\u200dğŸ’» Prathmesh: Iâ€™d focus on horizontal scaling by autoscaling EC2 instances behind a load balancer. I'd also optimize the model size and use Redis or similar tools for real-time caching. Transitioning to a microservices architecture would help separate model serving from data processing, improving response times and maintainability.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Excellent, Prathmesh! Thank you for the detailed responses. That wraps up the technical round.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Thank you! I really enjoyed discussing the project.\"),\n",
       "  np.float32(3712.024))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_score = db.similarity_search_with_score(query)\n",
    "docs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db6bbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_vec = embeds.embed_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaa7f776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f44c3015-d6f3-4c1d-a1e1-879da5c20e27', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: What were some of the technical challenges in this project?\\nğŸ§‘\\u200dğŸ’» Prathmesh: One challenge was managing memory usage and load timeâ€”since the model was 23GB, it took around 160 seconds to load initially. We used warm-started containers and lazy loading strategies to minimize latency. Handling the sparsity of item combinations was another challenge, which we addressed by using product-level fallbacks for recommendations.\\n\\n\\nTo deal with sparsity in SKU-based association rules, we created a fallback mechanism. If a userâ€™s basket had no matching SKU-level rules, we mapped those SKUs to their product categories or descriptions and used a secondary set of association rules trained at that level. This allowed us to preserve recommendation quality even in edge cases.â€'),\n",
       " Document(id='aa6f0617-b73c-4f7e-a062-0f416f4137ab', metadata={'source': 'text.txt'}, page_content=\"Sagemaker integration\\nWhen SageMaker starts a container, it will invoke the container with an argument of either\\xa0train\\xa0or\\xa0serve. We have set this container to accept the\\xa0serve\\xa0command which will, in turn, run our application.\\n* serve: The wrapper that starts the inference server. In most cases, you can use this file as-is.\\n* wsgi.py: The start up shell for the individual server workers. This only needs to be changed if you changed where predict.py is located or is named.\\n* predict.py: The algorithm-specific inference server. This is the file that you modify with your own algorithm's code.\\n* nginx.conf: Now included within the mdf-base-image. The configuration for the nginx master server that manages the multiple workers.\\nMock Interview Transcript â€“ NPTB Recommender System\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Hi Prathmesh, thanks for joining. Letâ€™s start with a quick overview of the NPTB recommender system you worked on at TCS.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Sure! NPTB, or Next Product to Buy, is a product recommendation engine built using association rule mining. Its goal is to suggest the next likely item a customer might purchase based on their current cart or historical transactions. It was developed to support the email marketing campaign use case and was deployed via Docker on AWS SageMaker.\"),\n",
       " Document(id='dbaecf8a-637a-40e3-b963-a898022a862c', metadata={'source': 'text.txt'}, page_content='ğŸ§‘\\u200dğŸ’¼ Interviewer: How was security and access control handled?\\nğŸ§‘\\u200dğŸ’» Prathmesh: All requests went through the API Gateway secured by TLS and authenticated via API keys. AWS IAM roles were used to restrict access between Lambda and SageMaker, and the EC2 instance was encrypted at rest. No public access was allowed to the internal Docker container.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: How did you evaluate model performance?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We used top-N hit rates as evaluation metricsâ€”specifically top-10 and top-2. These measure whether the actual next purchased item was present in the modelâ€™s top-N recommendations. With vehicle fitment filtering, the top-10 hit rate improved from ~35.7% to ~43.4%, showing strong gains.'),\n",
       " Document(id='20d36c7c-38d9-4083-80f1-f32e362a43e7', metadata={'source': 'text.txt'}, page_content=\"ğŸ§‘\\u200dğŸ’¼ Interviewer: What enhancements were proposed for NPTB?\\nğŸ§‘\\u200dğŸ’» Prathmesh: We planned to migrate from Pig scripts to PySpark for scalability, introduce OpenShift for container orchestration, and integrate higher-level product description-based rules to reduce sparsity. Also, we aimed to enable multi-channel support for website and mobile personalization.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Great! And finally, if you had to scale NPTB for a website with high traffic, what would you consider?\\nğŸ§‘\\u200dğŸ’» Prathmesh: Iâ€™d focus on horizontal scaling by autoscaling EC2 instances behind a load balancer. I'd also optimize the model size and use Redis or similar tools for real-time caching. Transitioning to a microservices architecture would help separate model serving from data processing, improving response times and maintainability.\\n\\nğŸ§‘\\u200dğŸ’¼ Interviewer: Excellent, Prathmesh! Thank you for the detailed responses. That wraps up the technical round.\\nğŸ§‘\\u200dğŸ’» Prathmesh: Thank you! I really enjoyed discussing the project.\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_by_vector(em_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2a52cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the vectore store to disk\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bc9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
